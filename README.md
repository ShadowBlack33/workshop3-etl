````markdown
# Workshop 3 â€” World Happiness (2015â€“2019) ğŸŒ  
**Streaming + ETL + ML + Data Warehouse + Power BI**

Proyecto completo de punta a punta basado en el World Happiness Report:

1. âœ… UnificaciÃ³n y limpieza de los 5 CSV (2015â€“2019).
2. âœ… EDA sÃ³lido para entender las variables clave.
3. âœ… Entrenamiento de un modelo de regresiÃ³n lineal mÃºltiple.
4. âœ… Data Streaming con Kafka: producer envÃ­a registros limpios + flags train/test.
5. âœ… Consumer aplica el modelo y carga resultados en un Data Warehouse PostgreSQL.
6. âœ… Tabla Ãºnica `predictions` lista para analÃ­tica y dashboard en Power BI.

Todo orquestado con: ğŸ **Python**, ğŸ³ **Docker**, ğŸ˜ **PostgreSQL**, ğŸ” **Kafka**, ğŸ“Š **Power BI** y ğŸ§© **VS Code**.

---

## 1. TecnologÃ­as usadas

- ğŸ **Python 3**
- ğŸ““ **Jupyter Notebooks** (`notebooks/EDA.ipynb`, `notebooks/ModelTraining.ipynb`)
- ğŸ” **Apache Kafka**
- ğŸ˜ **PostgreSQL** (Data Warehouse)
- ğŸ³ **Docker / docker-compose**
- ğŸ§© **VS Code** + extensiones:
  - Python
  - Jupyter
  - SQLTools + SQLTools PostgreSQL Driver
- ğŸ“Š **Power BI Desktop**
- ğŸ“¦ **scikit-learn**

---

## 2. Estructura del proyecto

```text
.
â”œâ”€ data/
â”‚  â”œâ”€ 2015.csv ... 2019.csv          # Datos crudos originales
â”œâ”€ db/
â”‚  â””â”€ pgdata/                        # Data de Postgres (montado por Docker)
â”œâ”€ docs/
â”‚  â””â”€ REPORT.md                      # Reporte del proyecto
â”œâ”€ kafka/
â”‚  â”œâ”€ producer.py                    # Producer de Kafka (stream de features)
â”‚  â””â”€ consumer.py                    # Consumer Kafka -> PostgreSQL
â”œâ”€ model/
â”‚  â””â”€ happiness_model.pkl            # Modelo entrenado (LinearRegression)
â”œâ”€ notebooks/
â”‚  â”œâ”€ EDA.ipynb                      # Limpieza + anÃ¡lisis + unificaciÃ³n
â”‚  â””â”€ ModelTraining.ipynb            # Experimentos de modelo
â”œâ”€ src/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ etl.py                         # LÃ³gica de ETL reutilizable
â”‚  â”œâ”€ train_model.py                 # Entrenamiento final y guardado del modelo
â”‚  â””â”€ evaluate.py (opcional)
â”œâ”€ docker-compose.yml
â”œâ”€ requirements.txt
â”œâ”€ .env
â””â”€ README.md
````

---

## 3. Flujo general ğŸ§ 

### 3.1 ETL + EDA (`notebooks/EDA.ipynb` + `src/etl.py`)

**Objetivo**
Unificar los 5 CSV, limpiar nombres de columnas, asegurar tipos numÃ©ricos y dejar las features listas para entrenamiento y streaming.

Pasos clave:

* Lectura de `2015.csv`â€“`2019.csv` desde `data/`.

* NormalizaciÃ³n de columnas:

  * `Country` / `Country or region` â†’ `Country`
  * `Score` / `Happiness Score` â†’ `Happiness Score`
  * `Economy (GDP per Capita)` / `GDP per capita` â†’ `GDP per capita`
  * `Health (Life Expectancy)` / `Healthy life expectancy` â†’ `Healthy life expectancy`
  * UnificaciÃ³n de columnas de apoyo social, libertad y corrupciÃ³n.

* ConstrucciÃ³n del dataframe unificado con:

  ```text
  Country, Year, Happiness Score,
  GDP per capita, Social support,
  Healthy life expectancy, Freedom,
  Perceptions of corruption
  ```

* AnÃ¡lisis exploratorio:

  * Distribuciones y estadÃ­sticas descriptivas.
  * Matriz de correlaciÃ³n entre features y felicidad.
  * Visualizaciones para comportamiento por aÃ±o y paÃ­s.

* No se aplica tratamiento agresivo de outliers (no aporta mejora clara); la decisiÃ³n se documenta.

ğŸ” **Rol del EDA**

* Justifica la selecciÃ³n de variables finales.
* Garantiza consistencia entre aÃ±os.
* Define la lÃ³gica de ETL centralizada en `src/etl.py`, usada tanto en entrenamiento como en streaming (sin â€œtrampasâ€ con datasets distintos).

---

### 3.2 Entrenamiento del modelo (`src/train_model.py`)

**Modelo**: `LinearRegression` (regresiÃ³n lineal mÃºltiple)

**Features finales**:

```python
FEATURES = [
    "GDP per capita",
    "Social support",
    "Healthy life expectancy",
    "Freedom",
    "Perceptions of corruption",
]
TARGET = "Happiness Score"
```

**LÃ³gica**:

1. Usa `load_unified()` de `src/etl.py`.
2. Filtra filas completas en `FEATURES + TARGET`.
3. Split 70/30 (`random_state=42`):

   * 70% â†’ train
   * 30% â†’ test
4. Entrena `LinearRegression` con el set de entrenamiento.
5. EvalÃºa sobre el **test set**:

   * RÂ²
   * MAE
   * RMSE
6. Guarda el modelo en `model/happiness_model.pkl`.

**Salida esperada (ejemplo)**:

```text
================ Model Training ================
Samples:
  Train: 329
  Test : 141

Performance on TEST set:
  RÂ²   : 0.804
  MAE  : 0.401
  RMSE : 0.532

Coefficients:
  GDP per capita            : 0.9160
  Social support            : 0.6781
  Healthy life expectancy   : 1.2863
  Freedom                   : 1.5349
  Perceptions of corruption : 0.9633

Model saved to: model/happiness_model.pkl
================================================
```

Demuestra que el modelo se entrena bien y se evalÃºa solo con datos no vistos.

---

## 4. Streaming con Kafka ğŸ”

### 4.1 Producer (`kafka/producer.py`)

**Responsabilidades**

* Leer los 5 CSV originales desde `data/`.
* Aplicar el **mismo ETL** que el EDA (`src/etl.py`).
* Reconstruir el split 70/30 con la misma lÃ³gica:

  * `is_train = 1`, `is_test = 0` para filas de entrenamiento.
  * `is_train = 0`, `is_test = 1` para filas de prueba.
* Agregar `y_true` (`Happiness Score`).
* Enviar cada registro al topic Kafka `happiness_features` como JSON.

Ejemplo de mensaje:

```json
{
  "Country": "France",
  "Year": 2018,
  "GDP per capita": 1.324,
  "Social support": 1.472,
  "Healthy life expectancy": 0.996,
  "Freedom": 0.450,
  "Perceptions of corruption": 0.183,
  "Happiness Score": 6.489,
  "is_train": 1,
  "is_test": 0
}
```

**Salida tÃ­pica**

```text
[producer] using topic=happiness_features @ localhost:9092
[producer] sending records...

â†’ (France, 2018, y_true=6.489, train=1, test=0)
â†’ (Brazil, 2019, y_true=6.300, train=0, test=1)
...

[producer] done. sent=781 â†’ topic=happiness_features @ localhost:9092
```

---

### 4.2 Consumer (`kafka/consumer.py`) â†’ PostgreSQL ğŸ˜

**Responsabilidades**

* Escuchar el topic `happiness_features`.
* Para cada mensaje:

  * Construir el vector de features.
  * Cargar `happiness_model.pkl`.
  * Calcular `y_pred`.
  * Hacer **UPSERT** en la tabla `predictions` en PostgreSQL:

```sql
CREATE TABLE IF NOT EXISTS predictions (
    country   TEXT    NOT NULL,
    year      INTEGER NOT NULL,
    gdp       REAL,
    social    REAL,
    health    REAL,
    freedom   REAL,
    corrupt   REAL,
    y_true    REAL,
    is_train  INTEGER,
    is_test   INTEGER,
    y_pred    REAL,
    UNIQUE(country, year, is_train, is_test)
);
```

* La constraint `UNIQUE` evita duplicados si se corre el pipeline varias veces.
* Se conserva:

  * Datos originales.
  * Flags de uso (`is_train`, `is_test`).
  * Valor real (`y_true`).
  * PredicciÃ³n (`y_pred`).

**Ejemplo de salida**

```text
[consumer] ready
  â€¢ topic:      happiness_features
  â€¢ bootstrap:  localhost:9092
  â€¢ group_id:   workshop3-consumer
  â€¢ model:      model/happiness_model.pkl
  â€¢ postgres:   workshop@localhost:5432/workshop3
  â€¢ table:      predictions
  â€¢ features:   GDP per capita, Social support, Healthy life expectancy, Freedom, Perceptions of corruption
--------------------------------------------------------------

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ France (2018)                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚RÂ²: 0.812    MAE: 0.320                                      â”‚
â”‚y_true: 6.489    y_pred: 6.402                               â”‚
â”‚train: 1    test: 0                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Brazil (2019)                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚RÂ²: 0.790    MAE: 0.410                                      â”‚
â”‚y_true: 6.300    y_pred: 6.052                               â”‚
â”‚train: 0    test: 1                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ“ upsert batch=200  total=200
âœ” finished. total_upserted=781
```

La tabla `predictions` queda lista para construir KPIs y visualizaciones en Power BI.

---

## 5. Notas de uso rÃ¡pido ğŸ§©

* Crear entorno virtual e instalar dependencias:

  ```bash
  python -m venv .venv
  .\.venv\Scripts\activate
  pip install -r requirements.txt
  ```

* Levantar servicios (Kafka, ZooKeeper, PostgreSQL) con `docker-compose.yml`.

* Usar **VS Code + SQLTools** para explorar la base:

  * Configurar conexiÃ³n PostgreSQL (`localhost:5432`, db `workshop3`, user `workshop`).
  * Consultar:

    ```sql
    SELECT COUNT(*) FROM predictions;
    SELECT * FROM predictions LIMIT 10;
    ```

* Power BI se conecta directamente a PostgreSQL sobre la tabla `predictions` para construir el dashboard de KPIs y performance del modelo.